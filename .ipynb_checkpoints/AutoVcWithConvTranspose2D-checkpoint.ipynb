{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only thing we want to change is the mode\n",
    "# however to do this, we have to run it using that script from the main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, pdb\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class LinearNorm(torch.nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, bias=True, w_init_gain='linear'):\n",
    "        super(LinearNorm, self).__init__()\n",
    "        self.linear_layer = torch.nn.Linear(in_dim, out_dim, bias=bias)\n",
    "\n",
    "        torch.nn.init.xavier_uniform_(\n",
    "            self.linear_layer.weight,\n",
    "            gain=torch.nn.init.calculate_gain(w_init_gain))\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear_layer(x)\n",
    "\n",
    "class ConvNorm1d(torch.nn.Module): \n",
    "    def __init__(self, in_channels, out_channels, kernel_size=1, stride=1, \n",
    "                 padding=None, dilation=1, bias=True, w_init_gain='linear'): \n",
    "        super(ConvNorm1d, self).__init__() \n",
    "        if padding is None: \n",
    "            assert(kernel_size % 2 == 1) \n",
    "            padding = int(dilation * (kernel_size - 1) / 2) \n",
    " \n",
    "        self.conv = torch.nn.Conv1d(in_channels, out_channels, \n",
    "                                    kernel_size=kernel_size, stride=stride, \n",
    "                                    padding=padding, dilation=dilation, \n",
    "                                    bias=bias) \n",
    " \n",
    "        torch.nn.init.xavier_uniform_( \n",
    "            self.conv.weight, gain=torch.nn.init.calculate_gain(w_init_gain)) \n",
    "\n",
    "    def forward(self, signal):\n",
    "        conv_signal = self.conv(signal)\n",
    "        return conv_signal\n",
    "\n",
    "class ConvNorm2d(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=1, stride=1,\n",
    "                 padding=None, dilation=1, bias=True, w_init_gain='linear'):\n",
    "        super(ConvNorm2d, self).__init__()\n",
    "        if padding is None:\n",
    "            assert(kernel_size % 2 == 1)\n",
    "            padding = int(dilation * (kernel_size - 1) / 2)\n",
    "\n",
    "        self.conv = torch.nn.Conv2d(in_channels, out_channels,\n",
    "                                    kernel_size=kernel_size, stride=stride,\n",
    "                                    padding=padding, dilation=dilation,\n",
    "                                    bias=bias)\n",
    "\n",
    "        torch.nn.init.xavier_uniform_(\n",
    "            self.conv.weight, gain=torch.nn.init.calculate_gain(w_init_gain))\n",
    "\n",
    "    def forward(self, signal):\n",
    "        conv_signal = self.conv(signal)\n",
    "        return conv_signal\n",
    "\n",
    "class ConvT2d(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=1, stride=1,\n",
    "                 padding=None, dilation=1, bias=True, w_init_gain='relu'):\n",
    "        super(ConvT2d, self).__init__()\n",
    "        if padding is None:\n",
    "            assert(kernel_size % 2 == 1)\n",
    "            padding = int(dilation * (kernel_size - 1) / 2)\n",
    "            \n",
    "        self.conv = torch.nn.ConvTranspose2d(in_channels, out_channels,\n",
    "                                    kernel_size=kernel_size, stride=stride,\n",
    "                                    padding=padding, dilation=dilation,\n",
    "                                    bias=bias)\n",
    "\n",
    "        torch.nn.init.xavier_uniform_(\n",
    "            self.conv.weight, gain=torch.nn.init.calculate_gain(w_init_gain)),\n",
    "\n",
    "    def forward(self, signal):\n",
    "        conv_signal = self.conv(signal)\n",
    "        return conv_signal \n",
    "\n",
    "# \"4.2. The Content Encoder\"\n",
    "class Encoder(nn.Module):\n",
    "    \"\"\"Encoder module:\n",
    "    \"\"\"\n",
    "    def __init__(self, dim_neck, dim_emb, freq):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.dim_neck = dim_neck\n",
    "        self.freq = freq\n",
    "        convolutions = []\n",
    "        for i in range(4):\n",
    "        # \"the input to the content encoder is the 80-dimensional mel-spectrogram of X1 concatenated with the speaker embedding\" - I think the embeddings are copy pasted from a dataset, as the Speaker Decoder is pretrained and may not actually appear in this implementation?\n",
    "            conv_layer = nn.Sequential(\n",
    "        # \"the input to the content encoder is the 80-dimensional mel-spectrogram of X1 concatenated with the speaker embedding. The concatenated features are fed into three 5 Ã— 1 convolutional layers, each followed by batch normalization and ReLU activation. The number of channels is 512\"\n",
    "                ConvNorm2d(1 if i==0 else 64 if i==1 else 128 if i==2 else 256,\n",
    "                         64 if i==0 else 128 if i==1 else 256 if i==2 else 512,\n",
    "                         kernel_size=3, stride=1,\n",
    "                         padding=1,\n",
    "                         dilation=1, w_init_gain='relu')\n",
    "                ,nn.BatchNorm2d(64 if i==0 else 128 if i==1 else 256 if i==2 else 512)\n",
    "                ,nn.ReLU()\n",
    "                ,nn.MaxPool2d((4,1))\n",
    "            )\n",
    "                \n",
    "            convolutions.append(conv_layer)\n",
    "        self.convolutions = nn.ModuleList(convolutions)\n",
    "\n",
    "        \n",
    "        # \"Both the forward and backward cell dimensions are 32, so their (LSTMs) combined dimension is 64.\"\n",
    "        self.lstm = nn.LSTM(512, dim_neck, 2, batch_first=True, bidirectional=True)\n",
    "\n",
    "        # c_org is speaker embedding\n",
    "    def forward(self, x, c_org):\n",
    "        #pdb.set_trace()\n",
    "        x = x.transpose(2,1).unsqueeze(1) # after this transpose, tensor should be shape (batch, feature, time)\n",
    "        # broadcasts c_org to a compatible shape to merge with x\n",
    "        c_org = c_org.unsqueeze(-1).expand(-1, -1, x.size(-1)).unsqueeze(1)\n",
    "        x = torch.cat((x, c_org), dim=2)\n",
    "        for conv in self.convolutions:\n",
    "            x = conv(x)\n",
    "        x = x.squeeze(2).transpose(-1,-2)\n",
    "        self.lstm.flatten_parameters()\n",
    "        # lstms output 64 dim\n",
    "        outputs, _ = self.lstm(x)\n",
    "        # backward is the first half of dimensions, forward is the second half\n",
    "        # pdb.set_trace()\n",
    "        out_forward = outputs[:, :, :self.dim_neck] #takes the first half of outputs\n",
    "        out_backward = outputs[:, :, self.dim_neck:]\n",
    "\n",
    "        # pdb.set_trace()\n",
    "        codes = []\n",
    "        \n",
    "        # for each timestep, skipping self.freq frames\n",
    "        for i in range(0, outputs.size(1), self.freq):\n",
    "            # remeber that i is self.freq, not increments of 1)\n",
    "            codes.append(torch.cat((out_forward[:,i+self.freq-1,:],out_backward[:,i,:]), dim=-1))\n",
    "        \n",
    "        # if self.freq is 32, then codes is a list of 4 tensors of size 64\n",
    "        return codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "config_path = '/homes/bdoc3/my_data/autovc_data/vte-autovc/model_saves/vte_autovcTest_dim16/config.pkl'\n",
    "config = pickle.load(open(config_path, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, pdb, pickle, argparse, shutil, yaml\n",
    "from data_loader import get_loader, pathSpecDataset\n",
    "from torch.backends import cudnn\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# For fast training.\n",
    "cudnn.benchmark = True\n",
    "\n",
    "with open(config.spmel_dir +'/spmel_params.yaml') as File:\n",
    "    spmel_params = yaml.load(File, Loader=yaml.FullLoader)\n",
    "vocalSet = pathSpecDataset(config, spmel_params)\n",
    "vocalSet_loader = DataLoader(vocalSet, batch_size=config.batch_size, shuffle=True, drop_last=False)\n",
    "# Data loader.\n",
    "#vcc_loader = get_loader(config)\n",
    "# pass dataloader and configuration params to Solver NN\n",
    "if config.file_name == 'defaultName' or config.file_name == 'deletable':\n",
    "    writer = SummaryWriter('testRuns/test')\n",
    "else:\n",
    "    writer = SummaryWriter(comment = '_' +config.file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Vt_Embedder(\n",
       "  (conv_layer1): Sequential(\n",
       "    (0): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (conv_layer2): Sequential(\n",
       "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): MaxPool2d(kernel_size=4, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (conv_layer3): Sequential(\n",
       "    (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): MaxPool2d(kernel_size=2, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (conv_layer4): Sequential(\n",
       "    (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): MaxPool2d(kernel_size=2, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (fc_layer1): Sequential(\n",
       "    (0): Linear(in_features=512, out_features=512, bias=True)\n",
       "    (1): Dropout(p=0, inplace=False)\n",
       "    (2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): ReLU()\n",
       "  )\n",
       "  (fc_layer2): Sequential(\n",
       "    (0): Linear(in_features=512, out_features=256, bias=True)\n",
       "    (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (fc_layer3): Sequential(\n",
       "    (0): Linear(in_features=512, out_features=64, bias=True)\n",
       "    (1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (fc_layer4): Sequential(\n",
       "    (0): Linear(in_features=384, out_features=256, bias=True)\n",
       "    (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (lstm): LSTM(256, 256, num_layers=2, batch_first=True, bidirectional=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "from vte_model import Vt_Embedder\n",
    "vte =  Vt_Embedder(config, spmel_params)\n",
    "for param in vte.parameters():\n",
    "    param.requires_grad = False\n",
    "vte_optimizer = torch.optim.Adam(vte.parameters(), 0.0001)\n",
    "vte_checkpoint = torch.load(config.emb_ckpt)\n",
    "new_state_dict = OrderedDict()\n",
    "for i, (key, val) in enumerate(vte_checkpoint['model_state_dict'].items()):\n",
    "    if key.startswith('class_layer'):\n",
    "        continue\n",
    "    new_state_dict[key] = val \n",
    "vte.load_state_dict(new_state_dict)\n",
    "\n",
    "for state in vte_optimizer.state.values():\n",
    "    for k, v in state.items():\n",
    "        if isinstance(v, torch.Tensor):\n",
    "            state[k] = v.cuda(which_cuda)\n",
    "\n",
    "device = torch.device(f'cuda:{config.which_cuda}' if torch.cuda.is_available() else 'cpu')\n",
    "vte.to(device)\n",
    "vte.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 192, 288])\n"
     ]
    }
   ],
   "source": [
    "hist_arr = np.array([0,0,0])\n",
    "# Print logs in specified order\n",
    "\n",
    "data_iter = iter(vocalSet_loader)\n",
    "x_real, style_idx, singer_idx = next(data_iter)\n",
    "\n",
    "x_real = x_real.to(device) \n",
    "\n",
    "x_real_chunked = x_real.view(x_real.shape[0]*config.chunk_num, x_real.shape[1]//config.chunk_num, -1)\n",
    "emb_org = vte(x_real_chunked)\n",
    "x = x_real\n",
    "\n",
    "# codes is a LIST of tensors \n",
    "encoder = Encoder(config.dim_neck, config.dim_emb, config.freq)\n",
    "encoder.to(device)\n",
    "\n",
    "codes = encoder(x, emb_org)\n",
    "# if no c_trg given, then just return the formatted encoder codes\n",
    "\n",
    "# list of reformatted codes        \n",
    "tmp = []\n",
    "for code in codes:\n",
    "    # reformatting tmp from list to tensor, and giving it new dim of 128 (x.size(1))\n",
    "    tmp.append(code.unsqueeze(1).expand(-1,int(x.size(1)/len(codes)),-1))\n",
    "code_exp = torch.cat(tmp, dim=1)\n",
    "# concat reformated encoder output with target speaker embedding\n",
    "encoder_outputs = torch.cat((code_exp, emb_org.unsqueeze(1).expand(-1,x.size(1),-1)), dim=-1)\n",
    "# shape should now be (2,192,320 (256+64)\n",
    "print(encoder_outputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 192, 288])\n",
      "torch.Size([2, 1, 512, 192])\n",
      "torch.Size([2, 256, 512, 192])\n"
     ]
    }
   ],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \"\"\"Decoder module:\n",
    "    \"\"\"\n",
    "    def __init__(self, dim_neck, dim_emb, dim_pre):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.lstm1 = nn.LSTM(dim_neck*2+dim_emb, dim_pre, 1, batch_first=True)\n",
    "        \n",
    "        convolutions = []\n",
    "        for i in range(3):\n",
    "            conv_layer = nn.Sequential(\n",
    "                ConvT2d(1 if i==0 else 64 if i==1 else 128 if i==2 else 256,\n",
    "                        64 if i==0 else 128 if i==1 else 256 if i==2 else 512,\n",
    "                        kernel_size=5,\n",
    "                        stride=1,\n",
    "                        padding=2),\n",
    "                nn.BatchNorm2d(64 if i==0 else 128 if i==1 else 256 if i==2 else 512))\n",
    "            convolutions.append(conv_layer) \n",
    "        self.convolutions = nn.ModuleList(convolutions)         \n",
    "\n",
    "        self.lstm2 = nn.LSTM(dim_pre, 1024, 2, batch_first=True)\n",
    "        self.linear_projection = LinearNorm(1024, 80)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #self.lstm1.flatten_parameters()\n",
    "        x, _ = self.lstm1(x)\n",
    "        x = x.transpose(1, 2)\n",
    "        \n",
    "        for conv in self.convolutions:\n",
    "            x = F.relu(conv(x))\n",
    "        x = x.transpose(1, 2)\n",
    "        \n",
    "        outputs, _ = self.lstm2(x)\n",
    "        \n",
    "        decoder_output = self.linear_projection(outputs)\n",
    "\n",
    "        return decoder_output\n",
    "\n",
    "decoder = Decoder(config.dim_neck, config.dim_emb, config.dim_pre)\n",
    "decoder.to(device)\n",
    "print(encoder_outputs.shape)\n",
    "outs, _ = decoder.lstm1(encoder_outputs)\n",
    "outs = outs.transpose(1, 2).unsqueeze(1)\n",
    "print(outs.shape)\n",
    "for conv in decoder.convolutions:\n",
    "    outs = F.relu(conv(outs))\n",
    "print(outs.shape)\n",
    "# outs, _ = decoder.lstm2(outs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 128, 169, 192])\n",
      "torch.Size([2, 64, 84, 192])\n",
      "torch.Size([2, 64, 84, 192])\n",
      "torch.Size([2, 64, 84, 192])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 80, 192])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Postnet(nn.Module):\n",
    "    \"\"\"Postnet\n",
    "        - Five 1-d convolution with 512 channels and kernel size 5\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Postnet, self).__init__()\n",
    "        self.convolutions = nn.ModuleList()\n",
    "\n",
    "        self.convolutions.append(\n",
    "            nn.Sequential(\n",
    "                ConvNorm2d(256, 128,\n",
    "                         kernel_size=5, stride=1,\n",
    "                         padding=(0,2),\n",
    "                         dilation=1, w_init_gain='tanh'),\n",
    "                nn.BatchNorm2d(128),\n",
    "                nn.MaxPool2d((3,1))),\n",
    "        )\n",
    "\n",
    "        for i in range(1, 5 - 1):\n",
    "            self.convolutions.append(\n",
    "                nn.Sequential(\n",
    "                    ConvNorm2d(128 if i==1 else 64,\n",
    "                             64,\n",
    "                             kernel_size=5, stride=1, padding=(1,2) if i<1 else 2,\n",
    "                             dilation=1, w_init_gain='tanh'),\n",
    "                    nn.BatchNorm2d(64),\n",
    "                    nn.MaxPool2d((2,1) if i==1 else (1,1))),\n",
    "            )\n",
    "\n",
    "        self.convolutions.append(\n",
    "            nn.Sequential(\n",
    "                ConvNorm2d(64, 1,\n",
    "                         kernel_size=5, stride=1,\n",
    "                         padding=(0,2),\n",
    "                         dilation=1, w_init_gain='linear'))\n",
    "\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)\n",
    "        for i in range(len(self.convolutions) - 1):\n",
    "            print(i)\n",
    "            x = torch.tanh(self.convolutions[i](x))\n",
    "\n",
    "        x = self.convolutions[-1](x)\n",
    "\n",
    "        return x    \n",
    "\n",
    "postnet = Postnet()\n",
    "postnet.to(device)\n",
    "\n",
    "x0 = torch.tanh(postnet.convolutions[0](outs))\n",
    "print(x0.shape)\n",
    "x1 = torch.tanh(postnet.convolutions[1](x0))\n",
    "print(x1.shape)\n",
    "x2 = torch.tanh(postnet.convolutions[2](x1))\n",
    "print(x2.shape)\n",
    "x3 = torch.tanh(postnet.convolutions[2](x2))\n",
    "print(x3.shape)\n",
    "\n",
    "x = postnet.convolutions[-1](x3)\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchenv",
   "language": "python",
   "name": "torchenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
